# Auto-generated NKI GEMM kernel
# Generated by CodeInliner
# Do not modify directly, instead, modify the generator source code


import neuronxcc.nki as nki
import neuronxcc.nki.language as nl
from neuronxcc.nki.typing import tensor

from autotune.modules.matmul import GEMMCompatibility
from autotune.typing import INPUT_TENSORS_DTYPE, KERNEL_KWARGS_DTYPE


def preprocessing(input_tensors: INPUT_TENSORS_DTYPE, kernel_kwargs: KERNEL_KWARGS_DTYPE):
    """
    Constraints:
        1. result_block initialization must occur before K loop and matmul operation
        2. matmul operation must occur after lhsT_block and rhs_block loads
        3. lhsT_block and rhs_block loads must be on the same side of K loop
        4. Loop order must contain exactly the characters 'M', 'N', and 'K'
    """

    mm = GEMMCompatibility(transposed_lhs=True)
    mm(input_tensors=input_tensors, kernel_kwargs=kernel_kwargs)
    loop_order = kernel_kwargs["loop_order"]
    tensor_positions = kernel_kwargs["tensor_positions"]
    if len(loop_order) != 3 or sorted(loop_order) != sorted("MNK"):
        raise ValueError(f"Invalid loop_order: {loop_order}. Must contain exactly M, N, and K.")
    M_position = loop_order.index("M")
    N_position = loop_order.index("N")
    K_position = loop_order.index("K")
    lhsT_block_position = tensor_positions["lhsT_block_position"]
    rhs_block_position = tensor_positions["rhs_block_position"]
    result_block_position = tensor_positions["result_block_position"]
    matmul_position = max(lhsT_block_position, rhs_block_position)
    assert (
        result_block_position <= K_position and result_block_position <= matmul_position
    ), f"result_block init must be before K loop and matmul. Received result_block_position {result_block_position}, K_position {K_position}, matmul_position {matmul_position}."
    assert (
        matmul_position <= lhsT_block_position and matmul_position <= rhs_block_position
    ), f"matmul must be after lhsT_block, rhs_block loads. Received matmul_position {matmul_position}, lhsT_block_position {lhsT_block_position}, rhs_block_position {rhs_block_position}."
    assert (lhsT_block_position <= K_position and rhs_block_position <= K_position) or (
        lhsT_block_position > K_position and rhs_block_position > K_position
    ), f"lhsT_block and rhs_block must be on the same side of K loop. Received lhsT_block_position {lhsT_block_position}, rhs_block_position {rhs_block_position}, K_position {K_position}."


@nki.jit
def lhsT_rhs_gemm_inlined_MKN(lhsT: tensor, rhs: tensor, NUM_BLOCK_M: int, NUM_BLOCK_N: int, NUM_BLOCK_K: int):
    """
    Auto-generated inlined lhsT (K, M) @ rhs (K, N) GEMM kernel that computes result = lhsT^T @ rhs.

    Loop structure:
    init_result_block
    loop_0: M
        loop_1: K
            init_rhs_block
            loop_2: N
                init_lhsT_block, matmul
    save_result

    This kernel uses block-based computation with a specific loop ordering.
    """

    # Setup mm compatibility object
    input_tensors = (lhsT, rhs)
    kernel_kwargs = {"NUM_BLOCK_M": NUM_BLOCK_M, "NUM_BLOCK_N": NUM_BLOCK_N, "NUM_BLOCK_K": NUM_BLOCK_K}
    preprocessing(input_tensors, kernel_kwargs)
    mm = GEMMCompatibility(transposed_lhs=True)
    mm(input_tensors, kernel_kwargs)
    result = nl.ndarray((mm.M, mm.N), dtype=lhsT.dtype, buffer=nl.shared_hbm)

    # Inlined operations at position -1 (before all loops)
    # Operations at position -1
    # Initialize result block
    result_block_shape = (
        mm.TILE_M,
        mm.NUM_BLOCK_M * mm.TILES_IN_BLOCK_M,
        mm.NUM_BLOCK_N * mm.TILES_IN_BLOCK_N,
        mm.TILE_N,
    )
    result_block = nl.zeros(result_block_shape, dtype=lhsT.dtype, buffer=nl.sbuf)

    return result
